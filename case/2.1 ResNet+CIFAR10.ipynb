{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "https://www.bilibili.com/video/BV18g4119737/?p=75&spm_id_from=pageDriver&vd_source=70200f7d09862fd682e5f89b22c89125\n",
    "\n",
    "# 1. CIFAR-10概述\n",
    "1. CIFAR-10（sai far ten）很小，教学很好用，10类*6k张照片\n",
    "2. 只有32*32的大小\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. LeNet-5\n",
    "1. ![](https://gitee.com/wyjyoga/my-pic-go/raw/master/img/202401202014124.png)\n",
    "## 3.1 几个注意点\n",
    "1. 分类问题使用CrossEntropyLoss，Regression用MSE\n",
    "2. `forward`函数中：\n",
    "    - `x = self.conv_unit(x)`实际上是`x = self.conv_unit.forward(x)`，只不过`nn.Module`帮我们做了这种事\n",
    "    - `logits`一般作为进入softmax之前的数值。即未经过归一化的概率\n",
    "        - 先有logits，而后通过sigmoid函数或者softmax函数得到概率 p 。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class Lenet5(nn.Module):\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super(Lenet5,self).__init__()\n",
    "\n",
    "        self.conv_unit = nn.Sequential(\n",
    "            # x : [32, 3, 32, 32]\n",
    "\n",
    "            nn.Conv2d(3,6,kernel_size=5,stride=1,padding=0),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2, padding=0),\n",
    "\n",
    "            nn.Conv2d(6,16,kernel_size = 5,stride=1,padding=0),\n",
    "            nn.AvgPool2d(kernel_size=2,stride=2,padding=0)\n",
    "        )\n",
    "\n",
    "        self.fc_unit = nn.Sequential(\n",
    "            nn.Linear(16*5*5, 120),     # 16*5*5是后来填写的\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120,84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84,10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # 分类问题使用CrossEntropyLoss\n",
    "        # Regression用MSE\n",
    "        self.criteon = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "\n",
    "        :param x:  [B, 3, 32, 32]\n",
    "        :return: [B, 10]\n",
    "        '''\n",
    "\n",
    "        # [B, 3, 32, 32] --> [B, 16, 5, 5]\n",
    "        x = self.conv_unit(x)\n",
    "        # [B, 16, 5, 5] --> [B, 16*5*5]\n",
    "        x = torch.flatten(x,start_dim=1)\n",
    "        # [B, 16*5*5] --> [B, 10]\n",
    "        # logits: 进入Softmax之前记为logits\n",
    "        logits = self.fc_unit(x)\n",
    "\n",
    "        pred = F.softmax(logits,dim=1)\n",
    "\n",
    "        # return logits\n",
    "        return pred\n",
    "        # loss = self.criteon(pred)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "net = Lenet5()\n",
    "tmp = torch.randn((32,3,32,32))\n",
    "out = net(tmp)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Main\n",
    "1. 技巧：我们希望data是normalized的形状\n",
    "    - 本质上是希望training 和 testing都具有很好且一致的分布\n",
    "    - 但是实际上属于作弊：提前看到了testing data中的数据\n",
    "    -"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "batchsz = 32\n",
    "EPOCH = 2\n",
    "device = torch.device('cuda')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    cifar_train = datasets.CIFAR10('cifar', True, transform=transforms.Compose([\n",
    "                    transforms.Resize((32,32)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "                ]), download=True)\n",
    "    cifar_train = DataLoader(cifar_train,batch_size = batchsz, shuffle = True)\n",
    "\n",
    "    cifar_test = datasets.CIFAR10('cifar', False, transform=transforms.Compose([\n",
    "                    transforms.Resize((32,32)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                        std=[0.229, 0.224, 0.225])\n",
    "                ]), download=True)\n",
    "    cifar_test = DataLoader(cifar_test,batch_size = batchsz, shuffle = True)\n",
    "\n",
    "    # # 读取第1个iter生成的迭代对象\n",
    "    # x, label = next(iter(cifar_train))\n",
    "    # print(x.shape)\n",
    "\n",
    "    model = Lenet5().to(device)\n",
    "    criteon = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    for epoch in range(EPOCH):\n",
    "        for batchidx, (x,lable) in enumerate(cifar_train):\n",
    "            x,lable = x.to(device), lable.to(device)\n",
    "            logits = model(x)\n",
    "            # logits 是还没有经过\n",
    "            loss = criteon(logits, lable)\n",
    "\n",
    "            optimizer.zero_grad()   # Init\n",
    "            loss.backward()         # Calculate\n",
    "            optimizer.step()        # Backprop\n",
    "\n",
    "        print(f\"epoch={epoch}, loss = {loss.detach().item()}\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        ######### Test\n",
    "        total_correct = 0\n",
    "        for x, label in cifar_test:\n",
    "            x,lable = x.to(device), lable.to(device)\n",
    "\n",
    "            # [B,10]\n",
    "            logits = model(x)\n",
    "            # [B]\n",
    "            pred = logits.argmax(dim=1)\n",
    "            total_correct = (pred == label).sum()\n",
    "            # total_correct = torch.eq(pred, label).float().sum()\n",
    "            print(f\"total_correct = {total_correct}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "epoch=0, loss = 2.142469644546509\n",
      "epoch=1, loss = 2.1181437969207764\n",
      "epoch=2, loss = 2.069254159927368\n",
      "epoch=3, loss = 2.264920234680176\n",
      "epoch=4, loss = 2.11747670173645\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_28032\\1759737772.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mmain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_28032\\478375326.py\u001B[0m in \u001B[0;36mmain\u001B[1;34m()\u001B[0m\n\u001B[0;32m     44\u001B[0m             \u001B[1;31m# [B]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     45\u001B[0m             \u001B[0mpred\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlogits\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0margmax\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdim\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 46\u001B[1;33m             \u001B[0mtotal_correct\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mpred\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0mlabel\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msum\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     47\u001B[0m             \u001B[1;31m# total_correct = torch.eq(pred, label).float().sum()\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     48\u001B[0m             \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"total_correct = {total_correct}\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "main()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "DRAFT DRAFT DRAFT DRAFT DRAFT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "28.0"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = 32\n",
    "padding = 0\n",
    "kernel = 5\n",
    "stride = 1\n",
    "(weight+2*padding-kernel)/stride + 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
