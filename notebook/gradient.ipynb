{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 0.\n",
    "https://www.bilibili.com/video/BV18g4119737?p=37&vd_source=70200f7d09862fd682e5f89b22c89125\n",
    "\n",
    "# 1. 梯度介绍\n",
    "## 1.1 几种情况\n",
    "1. local minimum ![](https://gitee.com/wyjyoga/my-pic-go/raw/master/img/20221111205923.png)\n",
    "\t- resnet通过“短路”的设置加深网络深度，从而达到很好的效果\n",
    "\t- 是因为加深之后，loss平面变得平滑（右侧），更加容易找到全局最优\n",
    "\n",
    "2. Saddel point\n",
    "![|325](https://gitee.com/wyjyoga/my-pic-go/raw/master/img/20221111210352.png)\n",
    "这里x和y是两个面，鞍点是x的极小值但是是y的最大值。这种情况是很多的\n",
    "\n",
    "## 1.2 Optimizer performance（影响优化的因素）\n",
    "1. Initialization status\n",
    "   ![](https://gitee.com/wyjyoga/my-pic-go/raw/master/img/20221111210747.png)\n",
    "   - 提到了**kaiming**初始方法\n",
    "2. learning rate\n",
    "   - 同时影响<u>速度和精度</u>\n",
    "   - 要衰减 decay\n",
    "3. momentum\n",
    "\n",
    "# 2. 激活函数&梯度\n",
    "1. sigmoid\n",
    "   - 缺陷：**梯度弥散**：当x比较大时，梯度很小，参数长时间得不到"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([-100.0000,  -77.7778,  -55.5556,  -33.3333,  -11.1111,   11.1111,\n          33.3333,   55.5556,   77.7778,  100.0000])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 返回一个1维张量，包含在区间start和end上均匀间隔的step个点。\n",
    "# 输出张量的长度由steps决定。\n",
    "# aka. 等差数列\n",
    "a = torch.linspace(-100,100,10)\n",
    "a"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.0000e+00, 1.6655e-34, 7.4564e-25, 3.3382e-15, 1.4945e-05, 9.9999e-01,\n        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(a)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Tanh：\n",
    "    - 导数也和tanh直接有关\n",
    "    - tanh = 2*sigmoid(2x)-1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([-0.7616, -0.6514, -0.5047, -0.3215, -0.1107,  0.1107,  0.3215,  0.5047,\n         0.6514,  0.7616])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.linspace(-1,1,10)\n",
    "torch.tanh(a)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. RELU\n",
    "    - 最简单最普遍最优先\n",
    "    - why work：x>=0时，**不会对梯度放大或者缩小**，梯度弥散和爆炸的情况得到很大缓解"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.0000, -0.7778, -0.5556, -0.3333, -0.1111,  0.1111,  0.3333,  0.5556,\n",
      "         0.7778,  1.0000])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1111, 0.3333, 0.5556, 0.7778,\n        1.0000])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(a)\n",
    "F.relu(a)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Loss函数&梯度\n",
    "## 3.1 MSE\n",
    "1. MSE = L2_norm^2 = torch.norm((y-(xw+b)),2)^2\n",
    "2. 注意：\n",
    "    - code时要对求导的变量设置为`requires_grad = True`或者使用`w.requires_grad_()`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([1.]),\n tensor([2.], requires_grad=True),\n tensor(1., grad_fn=<MseLossBackward0>))"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(1)\n",
    "w = torch.tensor([2.],requires_grad = True)\n",
    "# w = torch.full([1].float(),2)\n",
    "# w.requires_grad_()\n",
    "mse = F.mse_loss(torch.ones(1),x*w)\n",
    "x,w,mse"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. 手动算梯度be like: `autograd.grad()`\n",
    "    - 或者`[w1,w2,w3...]`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([2.]),)"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.grad(mse,[w])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. 大多数情况下不需要手动算梯度，直接调用：\n",
    "    - 此时不会返回grad值，而是默默记录在`w.grad`上\n",
    "    - 由于梯度发生累积，这里不再是2而是4(or more)\n",
    "    - 在实际中，会打印出梯度的`norm`来观察，毕竟梯度有时候可能dim很大"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.])\n",
      "<bound method Tensor.norm of tensor([6.])>\n"
     ]
    }
   ],
   "source": [
    "mse = F.mse_loss(torch.ones(1),x*w)\n",
    "mse.backward()\n",
    "print(w.grad)\n",
    "print(w.grad.norm)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Cross Entropy Loss\n",
    "1. 单分类多分类用的都很多\n",
    "2. 分类：\n",
    "    - binary\n",
    "    - multi-class\n",
    "    - softmax【本节】\n",
    "    - leave it to LR part\n",
    "3. softmax = soft version of max：\n",
    "    - 有一个（金字塔）放大作用：原来大score的现在（prob）更大\n",
    "    - 设 $softmax(\\alpha_j) = p_i$，则对$\\alpha_j$求导（j是第j个节点）\n",
    "        - 当$i=j,p_i(1-p_i)$，一定是个正值\n",
    "        - 当$i!=j, -p_j*p_i$，一定是个负值"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 0.8241,  0.0593, -0.1058], requires_grad=True)"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(3,requires_grad=True)\n",
    "a"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "必须指出维度"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.5376, 0.2502, 0.2122], grad_fn=<SoftmaxBackward0>)"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = F.softmax(a,dim=0)\n",
    "p"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. 求梯度时，必须指明size，否则会报错：**传入一个相同维度的张量即可**\n",
    "    - 而且下一行代码如果运行2遍会报错，必须用`retain_graph=True`指明说，计算图要保留，否则算了一次之后就销毁了"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-54-6eccf19331ed>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mones\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ma\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mD:\\Anaconda\\envs\\pythorch\\lib\\site-packages\\torch\\_tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    305\u001B[0m                 \u001B[0mcreate_graph\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    306\u001B[0m                 inputs=inputs)\n\u001B[1;32m--> 307\u001B[1;33m         \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    308\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    309\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda\\envs\\pythorch\\lib\\site-packages\\torch\\autograd\\__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    154\u001B[0m     Variable._execution_engine.run_backward(\n\u001B[0;32m    155\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 156\u001B[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001B[0m\u001B[0;32m    157\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    158\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "p.backward(torch.ones(a.shape))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. 注意这里不能直接传入p去求grad，当对loss求导时，loss一定是一个scalar，不能是一个vector\n",
    "    - p是vector，p[1]是scalar。当对p[1]求导，返回值是一个dim为1长度为3的，表明这个loss对3个params的更新\n",
    "    - 当p[1]对3个params求导时，i=j时为正否则为负"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5376, 0.2502, 0.2122], grad_fn=<SoftmaxBackward0>)\n",
      "当p[1]对3个params求导时，i=j时为正否则为负\n"
     ]
    },
    {
     "data": {
      "text/plain": "(tensor([-0.1345,  0.1876, -0.0531]),)"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = F.softmax(a,dim=0)\n",
    "print(p)\n",
    "print(\"当p[1]对3个params求导时，i=j时为正否则为负\")\n",
    "torch.autograd.grad(p[1],[a],retain_graph=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
