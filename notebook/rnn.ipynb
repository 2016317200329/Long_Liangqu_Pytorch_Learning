{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 0.\n",
    "1. https://www.bilibili.com/video/BV18g4119737?p=87&spm_id_from=pageDriver&vd_source=70200f7d09862fd682e5f89b22c89125\n",
    "\n",
    "# 1. rnn layer\n",
    "## 1.1 构建一个rnn layer\n",
    "1. 两个参数：\n",
    "    - `input`: input seq的shape。这里是一个长度为100的word vec,准确的说是“number of input vector”\n",
    "    - `h_0`: mem 大小，默认初始化为0。这里是一个长度为10的memory,准确的说是“number of hidden state”\n",
    "    - 第3个参数是“nums of layer”, 默认=1\n",
    "2. `rnn._parameters.keys()`：查看网络层名称，这里有`l0`的意思是，它也可以像cnn一样有多层，但是不会很长，因为它主要在时间上进行展开\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['weight_ih_l0', 'weight_hh_l0', 'bias_ih_l0', 'bias_hh_l0'])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "rnn = nn.RNN(100,10)\n",
    "# 查看网络层名称\n",
    "print(rnn._parameters.keys())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. 展示weight\n",
    "    - `weight_ih_l0`连接的是input层和hidden layer,当input也就是$x_t$的shape是`(100,)`时，由$x_t@w_{ih}.T$可得，$w_ih$的shape是`[10, 100]`\n",
    "    - `weight_hh_l0`链接的是 $h_0$ 和hidden，为了方便计算一般$h_0$的大小会设置成和hidden len的大小。所以$w_hh$的shape是`[10,10]`\n",
    "4. 展示bias\n",
    "    - bias直接是hidden len=10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10]) torch.Size([10, 100])\n",
      "torch.Size([10]) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(rnn.weight_hh_l0.shape,rnn.weight_ih_l0.shape)\n",
    "print(rnn.bias_hh_l0.shape,rnn.bias_ih_l0.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 forward函数\n",
    "1. `x`：shape=[seq len, b, word vec]，比如是[5,3,100]，3句话，每句话5个单词，每个单词100dim\n",
    "2. `h0`,`ht`: shape=[num layers, b, h dim]，比如是[1,3, 10]\n",
    "    - 1是上面定义的lstm单元的层数\n",
    "    - b是batch size，比如这里可以是3\n",
    "    - 10是上面hidden units的个数\n",
    "    - h0是输入，ht是t时刻的输出\n",
    "3. `out` ：shape=[seq len, b, h dim],比如是[5,3, 10]\n",
    "4. `ht`和`out`的区别：\n",
    "    - 前者是最后一个时刻的结果\n",
    "    - out是运行完了所有时刻/ 所有单词后的“聚合”的结果，是[h1,h2,...,h5]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "out, ht = forward(x, h0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Application\n",
    "## 2.1 Singel layer RNN\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(100, 10)\n"
     ]
    }
   ],
   "source": [
    "rnn = nn.RNN(100,10,1)\n",
    "print(rnn)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 10]) torch.Size([1, 3, 10])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10, 3, 100)\n",
    "out, h = rnn(x, torch.zeros(1,3,10))\n",
    "print(out.shape, h.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 multi-layer RNN\n",
    "1. 多层的话，从第2层开始，要考虑上一层的输出\n",
    "2. 第一层是l0，第二层是l1\n",
    "    - 第一层把input的embedding-->10 dim\n",
    "    - 第2层接收第一层的输出，"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(100, 10, num_layers=2)\n",
      "odict_keys(['weight_ih_l0', 'weight_hh_l0', 'bias_ih_l0', 'bias_hh_l0', 'weight_ih_l1', 'weight_hh_l1', 'bias_ih_l1', 'bias_hh_l1'])\n"
     ]
    }
   ],
   "source": [
    "rnn = nn.RNN(100,10,2)\n",
    "print(rnn)\n",
    "print(rnn._parameters.keys())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([10, 100]), torch.Size([10, 10]))"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.weight_ih_l0.shape,rnn.weight_hh_l0.shape\n",
    "# [10,100]表明ih是一个100-->10的transition,输入是len=100的vector\n",
    "# [10,10]表明是一个10-->10的transition，要输出10个单词的表示"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([10, 10]), torch.Size([10, 10]))"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.weight_ih_l1.shape, rnn.weight_hh_l1.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 4层的"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(100, 20, num_layers=4)\n"
     ]
    }
   ],
   "source": [
    "rnn = nn.RNN(100,20,4)\n",
    "print(rnn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- h是最后的时间戳上，所有的状态，所以包括4层的结果：[4,3,20]；\n",
    "- out中是对10个单词，每个单词取最后一个状态。\n",
    "- out是最后的时间戳伤，最后的状态，所以是[10, 3, 20],是3句话上10个单词的pred结果"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 20]) torch.Size([4, 3, 20])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10, 3, 100)\n",
    "out, h = rnn(x)  # h0的shape=[4, 3, 20]\n",
    "print(out.shape, h.shape)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 定长的输出：我们的问题是[300, B, 3] ---> [, B, T_Global]\n",
    "    - T_Global是max(T_i) among all known T_i\n",
    "- 不定长的输出:我们的问题是[300, B, 3] ---> [, B, T_i]\n",
    "- 问题1：\n",
    "    - 这种应用是否常规？比如有一个dim（embedding）其实和时间无关，直接不用？\n",
    "    - 不常规的用法是否会遭到质疑？\n",
    "    - 另外输出是一个分布，是否意味着我需要把它归一化？\n",
    "-"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. RNNCEll\n",
    "1. 其实就是自定义每个单词怎么处理"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 100])\n",
      "torch.Size([3, 20])\n",
      "torch.Size([3, 100])\n",
      "torch.Size([3, 20])\n",
      "torch.Size([3, 100])\n",
      "torch.Size([3, 20])\n",
      "torch.Size([3, 100])\n",
      "torch.Size([3, 20])\n",
      "torch.Size([3, 100])\n",
      "torch.Size([3, 20])\n",
      "torch.Size([3, 100])\n",
      "torch.Size([3, 20])\n",
      "torch.Size([3, 100])\n",
      "torch.Size([3, 20])\n",
      "torch.Size([3, 100])\n",
      "torch.Size([3, 20])\n",
      "torch.Size([3, 100])\n",
      "torch.Size([3, 20])\n",
      "torch.Size([3, 100])\n",
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10, 3, 100)\n",
    "\n",
    "cell1 = nn.RNNCell(100,20)  #\n",
    "h1 = torch.zeros(3,20)\n",
    "# 循环10次，从xt-->h1\n",
    "for xt in x:\n",
    "    print(xt.shape)\n",
    "    h1 = cell1(xt,h1)\n",
    "    print(h1.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 two-layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "x = torch.randn(10, 3, 100)\n",
    "\n",
    "cell1 = nn.RNNCell(100,30)      # layer1: 把100的input-->30的mem\n",
    "cell2 = nn.RNNCell(30,20)       # layer2: 把30的mem-->20的mem\n",
    "h1 = torch.zeros(3, 30)\n",
    "h2 = torch.zeros(3, 20)\n",
    "\n",
    "for xt in x:\n",
    "    h1 = cell1(xt,h1)\n",
    "    h2 = cell2(h1,h2)           # layer2取决于上一个mem的输出，以及自己上一轮的输出"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# padding实验"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "data": {
      "text/plain": "Parameter containing:\ntensor([[[[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.],\n          [ 9., 10., 11., 12., 13., 14., 15., 16.],\n          [17., 18., 19., 20., 21., 22., 23., 24.],\n          [25., 26., 27., 28., 29., 30., 31., 32.],\n          [33., 34., 35., 36., 37., 38., 39., 40.],\n          [41., 42., 43., 44., 45., 46., 47., 48.],\n          [49., 50., 51., 52., 53., 54., 55., 56.],\n          [57., 58., 59., 60., 61., 62., 63., 64.]]]], dtype=torch.float64,\n       requires_grad=True)"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = torch.nn.Parameter(torch.reshape(torch.range(1,16),(1,1,4,4)))\n",
    "\n",
    "x = torch.nn.Parameter(torch.reshape(torch.arange(1,8*8+1,dtype=torch.double),(1,1,8,8)))\n",
    "conv2 = torch.nn.Conv2d(1,1,(3,3),(4,4),1,bias=False)\n",
    "x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "conv2.weight = torch.nn.Parameter(torch.ones(1,1,3,3,dtype=torch.double))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "Parameter containing:\ntensor([[[[1., 1., 1.],\n          [1., 1., 1.],\n          [1., 1., 1.]]]], dtype=torch.float64, requires_grad=True)"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2.weight"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[ 22.,  54.],\n          [201., 333.]]]], dtype=torch.float64, grad_fn=<ConvolutionBackward0>)"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANS = conv2(x)\n",
    "ANS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 1, 4, 4])"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANS.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "54"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4+5+6+12+13+14"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
