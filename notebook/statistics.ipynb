{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 0. intro\n",
    "1. https://www.bilibili.com/video/BV18g4119737/?p=31&spm_id_from=pageDriver&vd_source=70200f7d09862fd682e5f89b22c89125\n",
    "2. 统计属性：\n",
    "    - norm\n",
    "    - mean sum\n",
    "    - prod\n",
    "    - max,min,argmin,argmax\n",
    "    - kthvalue,topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. 范数norm\n",
    "1. 是范数不是正则化\n",
    "2. matrix norm和vector norm\n",
    "    - 多用后者"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b:tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "c:tensor([[[1., 1.],\n",
      "         [1., 1.]],\n",
      "\n",
      "        [[1., 1.],\n",
      "         [1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "a=torch.full([8],1.)\n",
    "b=a.view(2,4)\n",
    "print(f\"b:{b}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor(8.), tensor(8.), tensor(8.))"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.norm(1),b.norm(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor(2.8284), tensor(2.8284), tensor(2.8284), 2.8284271247461903)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#8**(0.5)\n",
    "a.norm(2),b.norm(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([4., 4.])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.norm(1,dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([2., 2.])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.norm(2,dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. 常用运算\n",
    "1. `prod`:返回给定维度 dim 中 input 张量的每一行的乘积。\n",
    "2. `argmax` `argmin`返回的是下标\n",
    "    - 而且会先把`a`打平成一个vector,然后返回索引\n",
    "    - 不希望打平需要参数`dim`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0., 1., 2., 3.],\n        [4., 5., 6., 7.]])"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.arange(8).view(2,4).float()\n",
    "a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor(0.), tensor(7.), tensor(3.5000), tensor(0.), tensor(28.))"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.min(),a.max(),a.mean(),a.prod(),a.sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor(0), tensor(7))"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.argmin(),a.argmax()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tensor(3)\n",
      "tensor([3, 1, 7, 5])\n"
     ]
    }
   ],
   "source": [
    "a=torch.randn(4,10)\n",
    "print()\n",
    "print(a.argmax())\n",
    "print(a.argmax(dim=1))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. dim和keepdim\n",
    "1. `a`表示4张照片,10个分类,因此一个照片对应了10个预测置信度,此时要寻找最大的分类是哪个及其对应的概率值\n",
    "2. `keepdim=True`时,并不做`squeeze`的操作"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.0155, -1.9601, -0.6398, -0.2628,  0.8060, -0.2826, -0.4652,  0.4693,\n",
      "          1.8985, -0.1764],\n",
      "        [ 0.4289,  1.0759, -1.7544,  1.5490, -1.9964, -0.2590, -0.2805,  0.0518,\n",
      "          1.7375, -1.4277],\n",
      "        [ 0.9108,  0.5244, -1.6860,  1.7950, -1.7996, -0.8249,  0.1810,  0.4719,\n",
      "         -0.5351, -1.2075],\n",
      "        [-0.2678, -0.3797,  1.8906, -1.7042,  0.2277,  2.1196,  0.0288,  0.9136,\n",
      "          1.1492, -0.3673]])\n"
     ]
    },
    {
     "data": {
      "text/plain": "torch.return_types.max(\nvalues=tensor([1.8985, 1.7375, 1.7950, 2.1196]),\nindices=tensor([8, 8, 3, 5]))"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.randn(4,10)\n",
    "print(a)\n",
    "# 前面的数字是value后面的数字是下标\n",
    "print(a.max(dim=1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "此时a.max或者a.argmax的返回值的shape是："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([4])"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.argmax(dim=1).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "但是如果`keepdim=True`,就会是:,并不做`squeeze`的操作"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "a.argmax(dim=1,keepdim=True).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([4, 1])"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Top-k or k-th\n",
    "1. `topk`得到最大的`n`个值及其下标\n",
    "    - 最小的几个需要`largest=False`\n",
    "2. `kthvalue`得到第`n`小的值,**而且只能获得“小”**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.0155, -1.9601, -0.6398, -0.2628,  0.8060, -0.2826, -0.4652,  0.4693,\n",
      "          1.8985, -0.1764],\n",
      "        [ 0.4289,  1.0759, -1.7544,  1.5490, -1.9964, -0.2590, -0.2805,  0.0518,\n",
      "          1.7375, -1.4277],\n",
      "        [ 0.9108,  0.5244, -1.6860,  1.7950, -1.7996, -0.8249,  0.1810,  0.4719,\n",
      "         -0.5351, -1.2075],\n",
      "        [-0.2678, -0.3797,  1.8906, -1.7042,  0.2277,  2.1196,  0.0288,  0.9136,\n",
      "          1.1492, -0.3673]])\n",
      "torch.return_types.topk(\n",
      "values=tensor([[1.8985, 0.8060, 0.4693],\n",
      "        [1.7375, 1.5490, 1.0759],\n",
      "        [1.7950, 0.9108, 0.5244],\n",
      "        [2.1196, 1.8906, 1.1492]]),\n",
      "indices=tensor([[8, 4, 7],\n",
      "        [8, 3, 1],\n",
      "        [3, 0, 1],\n",
      "        [5, 2, 8]]))\n",
      "torch.return_types.topk(\n",
      "values=tensor([[-2.0155, -1.9601, -0.6398],\n",
      "        [-1.9964, -1.7544, -1.4277],\n",
      "        [-1.7996, -1.6860, -1.2075],\n",
      "        [-1.7042, -0.3797, -0.3673]]),\n",
      "indices=tensor([[0, 1, 2],\n",
      "        [4, 2, 9],\n",
      "        [4, 2, 9],\n",
      "        [3, 1, 9]]))\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(a.topk(3,dim=1))\n",
    "print(a.topk(3,dim=1,largest=False))\n",
    "print(a.kthvalue(8,dim=1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. compare\n",
    "1. \\>, gt(a,0), a!=0, eq(a,b) 都是对每个位置元素的比较\n",
    "2. 常用判断两个是否相等：`torch.all(torch.eq(a,b)`返回的是一个true or false的值\n",
    "\n",
    "# 6. 有助于理解dim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "b = torch.tensor([1.,2.,3.,4.,5.,6.])\n",
    "b = b.view(2,3)\n",
    "b.norm(2,dim=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}