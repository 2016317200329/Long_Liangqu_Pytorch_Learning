{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 0. intro\n",
    "1. https://www.bilibili.com/video/BV18g4119737?p=29&vd_source=70200f7d09862fd682e5f89b22c89125\n",
    "2. 介绍了：\n",
    "    - 基本四则运算\n",
    "    - element-wise乘法。nn的降维/线性层常用\n",
    "    - 还有"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. basic\n",
    "1. 加减乘除,**一些写法是等效的**：\n",
    "    - 加：+或者sum\n",
    "    - 减：-或者sub\n",
    "    - 乘：*或者mul\n",
    "    - 除：/或者div\n",
    "2. 有的会进行广播,比如："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a+b:\n",
      " tensor([[1.0658, 1.2585, 1.0679, 1.1287],\n",
      "        [1.1720, 1.2874, 0.5183, 1.3710],\n",
      "        [1.2377, 1.5517, 0.9622, 1.3792]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(3,4)\n",
    "b = torch.rand(4)\n",
    "print(\"a+b:\\n\",a + b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. 可以检查一下等效效果："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(True)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(torch.eq(a-b, torch.sub(a,b)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(True)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(torch.eq(a*b, torch.mul(a,b)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(True)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(torch.eq(a/b, torch.div(a,b)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. element-wise乘法\n",
    "1. `torch.mm`:\n",
    "    - **element-wise**\n",
    "    - only for 2d,所以不推荐\n",
    "2. `torch.matmul` == `@`\n",
    "    - **element-wise**\n",
    "    - 推荐用这个,或者直接用@,是matmul的重载\n",
    "    -"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "a = torch.tensor([[3.,3.],[3.,3.]])\n",
    "b = torch.ones(2,2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mm: tensor([[6., 6.],\n",
      "        [6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "c_mm = torch.mm(a,b)\n",
    "print(\"mm:\",c_mm)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matmul or @: tensor([[6., 6.],\n",
      "        [6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "c_matmul = torch.matmul(a,b)\n",
    "print(\"matmul or @:\",c_matmul)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. example: 降维,nn中对线性层会用\n",
    "    - 先用`view`函数打平成(4,784)的向量`x`,然后通过降维,得到一个(4,512)\n",
    "    - 很显然“降维”通过矩阵乘法完成,`x`需要和(784,512)大小的矩阵`w`相乘\n",
    "    - **注意pytorch的写法**：`w`的**第一个维度**是`chanel-out`。因此一般用`.t()`转置一下"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([4, 512])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(4,784)\n",
    "w = torch.rand(512,784)\n",
    "# 降维\n",
    "(x@w.t()).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. 高维的mutmul\n",
    "    - 我们希望的效果是对batch和channel这两个维度保持不变,对H\\*W这个大小的矩阵进行一个 **“并行”** 的相乘\n",
    "    -"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28,64)*(64,32)得到的shape: torch.Size([4, 3, 28, 32])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(4,3,28,64)\n",
    "b = torch.rand(4,3,64,32)\n",
    "ans = torch.matmul(a,b)\n",
    "print(\"(28,64)*(64,32)得到的shape:\",ans.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "也可以使用broadcast机制,对`b`进行广播"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# broadcast\n",
    "a = torch.rand(4,3,28,64)\n",
    "b = torch.rand(4,1,64,32)\n",
    "ans = torch.matmul(a,b)\n",
    "ans.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. 其他运算\n",
    "1. `pow`和`**`的效果差不多\n",
    "2. `.exp`和`log`互为反函数,注意后者是`e`为底,以2为底使用`log2`\n",
    "## 3.1 近似\n",
    "1. floor() ceil(). round()是四舍五入\n",
    "2. trunc(),frac()分别裁为整数和小数部分"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.trunc(): tensor(3.)\n",
      "a.frac(): tensor(0.1400)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(3.14)\n",
    "print(\"a.trunc():\",a.trunc())\n",
    "print(\"a.frac():\",a.frac())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. 常用的是gradient clipping的操作：`clamp(threshold)`\n",
    "    - 当训练时,梯度不稳定,一定要打印一下**梯度的模**：`w.grad.norm(2)`,**一般100或者1000都很大,小于10是比较合适的**\n",
    "    - 所以要对梯度进行限制（不是对w进行限制！）"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad:\n",
      " tensor([[2.8222, 7.4594, 5.4390],\n",
      "        [7.9058, 7.4556, 3.6127]])\n",
      "从10截断\n",
      "grad.clamp(10):\n",
      " tensor([[10., 10., 10.],\n",
      "        [10., 10., 10.]])\n"
     ]
    }
   ],
   "source": [
    "grad = torch.rand(2,3)*15\n",
    "# print(f\"{grad.max()},{grad.median()}\")\n",
    "print(\"grad:\\n\",grad)\n",
    "print(\"从10截断\")\n",
    "print(\"grad.clamp(10):\\n\",grad.clamp(10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}